{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mat Scalpello Dundee University MSc data Science Project 2020\n",
    "#Orignal Code from Graph Algorithms - Mark Needham and Amy Hodler 2019 (O'Reilly Media)\n",
    "#Modifications by Mat Scalpello as indicated\n",
    "# V8 can also be used for models V10-12\n",
    "# Added surname community detection features\n",
    "# Same as Version three except using connectionID instead of nodeID (which changes)\n",
    "# Added community detection features - label prop and louvain\n",
    "# Added triangles and clustering coefficient\n",
    "# Used largest Louvain community "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from py2neo import Graph\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "graph = Graph(\"bolt://localhost\", auth=(\"neo4j\", \"123\"))\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to down sample negative examples\n",
    "#MSCALPELLO - Changed downsample function to be able to specify a percentage of negative links required V10-12\n",
    "def down_sample(df):\n",
    "    dscopy = df.copy()\n",
    "    \n",
    "    negatives = Counter(dscopy.label.values)[0]\n",
    "    positives = Counter(dscopy.label.values)[1]\n",
    "    samplesize = round(negatives - (positives))\n",
    "    #Use this one for 60%\n",
    "    #samplesize = round(negatives - (positives*.6))\n",
    "    #Use this one for 80%\n",
    "    #samplesize = round(negatives - (positives*.8))\n",
    "                       \n",
    "    dscopy = dscopy.drop(dscopy[dscopy.label == 0].sample(n=samplesize, random_state=1).index)\n",
    "    return dscopy.sample(frac=1)\n",
    "\n",
    "def evaluate_model(predictions, actual):\n",
    "    return pd.DataFrame({\n",
    "        \"Measure\": [\"Accuracy\", \"Precision\", \"Recall\"],\n",
    "        \"Score\": [accuracy_score(actual, predictions), \n",
    "                  precision_score(actual, predictions), \n",
    "                  recall_score(actual, predictions)]\n",
    "    })\n",
    "\n",
    "def feature_importance(columns, classifier):        \n",
    "    display(\"Feature Importance\")\n",
    "    df = pd.DataFrame({\n",
    "        \"Feature\": columns,\n",
    "        \"Importance\": classifier.feature_importances_\n",
    "    })\n",
    "    df = df.sort_values(\"Importance\", ascending=False)    \n",
    "    ax = df.plot(kind='bar', x='Feature', y='Importance', legend=None)\n",
    "    ax.xaxis.set_label_text(\"\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSCALPELLO Modified to use connectionid and an appropriate index\n",
    "# Using NEO4J IDs does not work consistently\n",
    "# Added Resource allocation and Adamic Adar\n",
    "def create_LP_features(data, rel_type):\n",
    "    query = \"\"\"\n",
    "    UNWIND $pairs AS pair\n",
    "    MATCH (p1:Person) \n",
    "    USING INDEX p1:Person(connectionid) \n",
    "    WHERE p1.connectionid = pair.node1\n",
    "    MATCH (p2:Person) \n",
    "    USING INDEX p2:Person(connectionid) \n",
    "    WHERE p2.connectionid = pair.node2\n",
    "    RETURN pair.node1 AS node1,\n",
    "           pair.node2 AS node2,\n",
    "           algo.linkprediction.commonNeighbors(\n",
    "               p1, p2, {relationshipQuery: $relType}) AS cn,\n",
    "           algo.linkprediction.adamicAdar(\n",
    "               p1, p2, {relationshipQuery: $relType}) AS aa,\n",
    "           algo.linkprediction.preferentialAttachment(\n",
    "               p1, p2, {relationshipQuery: $relType}) AS pa,\n",
    "           algo.linkprediction.totalNeighbors(\n",
    "               p1, p2, {relationshipQuery: $relType}) AS tn,\n",
    "           algo.linkprediction.resourceAllocation(\n",
    "               p1, p2, {relationshipQuery: $relType}) AS ra\n",
    "               \n",
    "    \"\"\"\n",
    "    pairs = [{\"node1\": node1, \"node2\": node2}  for node1,node2 in data[[\"node1\", \"node2\"]].values.tolist()]\n",
    "    features = graph.run(query, {\"pairs\": pairs, \"relType\": rel_type}).to_data_frame()\n",
    "    features = features.drop_duplicates()\n",
    "    return pd.merge(data, features, on = [\"node1\", \"node2\"])\n",
    "\n",
    "# MSCALPELLO Modified to use connectionid and an appropriate index\n",
    "# Using NEO4J IDs does not work consistently\n",
    "def create_community_features(data, triangles_prop, coefficient_prop):\n",
    "    query = \"\"\"\n",
    "    UNWIND $pairs AS pair\n",
    "    MATCH (p1:Person) \n",
    "    USING INDEX p1:Person(connectionid) \n",
    "    WHERE p1.connectionid = pair.node1\n",
    "    MATCH (p2:Person) \n",
    "    USING INDEX p2:Person(connectionid) \n",
    "    WHERE p2.connectionid = pair.node2\n",
    "    RETURN pair.node1 AS node1,\n",
    "    pair.node2 AS node2,\n",
    "    apoc.coll.min([p1[$trianglesProp], p2[$trianglesProp]]) AS minT,\n",
    "    apoc.coll.max([p1[$trianglesProp], p2[$trianglesProp]]) AS maxT,\n",
    "    apoc.coll.min([p1[$coefficientProp], p2[$coefficientProp]]) AS minC,\n",
    "    apoc.coll.max([p1[$coefficientProp], p2[$coefficientProp]]) AS maxC\n",
    "    \"\"\"    \n",
    "    pairs = [{\"node1\": node1, \"node2\": node2}  for node1,node2 in data[[\"node1\", \"node2\"]].values.tolist()]    \n",
    "    params = {\n",
    "    \"pairs\": pairs,\n",
    "    \"trianglesProp\": triangles_prop,\n",
    "    \"coefficientProp\": coefficient_prop\n",
    "    }\n",
    "    features = graph.run(query, params).to_data_frame()\n",
    "    features = features.drop_duplicates()\n",
    "    return pd.merge(data, features, on = [\"node1\", \"node2\"])\n",
    "\n",
    "# MSCALPELLO Modified to use connectionid and an appropriate index\n",
    "# Using NEO4J IDs does not work consistently\n",
    "def communitydetection_features(data, partition_prop, louvain_prop):\n",
    "    query = \"\"\"\n",
    "    UNWIND $pairs AS pair\n",
    "    MATCH (p1:Person) \n",
    "    USING INDEX p1:Person(connectionid) \n",
    "    WHERE p1.connectionid = pair.node1\n",
    "    MATCH (p2:Person) \n",
    "    USING INDEX p2:Person(connectionid) \n",
    "    WHERE p2.connectionid = pair.node2\n",
    "    RETURN pair.node1 AS node1,\n",
    "    pair.node2 AS node2,\n",
    "    algo.linkprediction.sameCommunity(p1, p2, $partitionProp) AS lp,    \n",
    "    algo.linkprediction.sameCommunity(p1, p2, $louvainProp) AS lv\n",
    "    \"\"\"\n",
    "    pairs = [{\"node1\": node1, \"node2\": node2}  for node1,node2 in data[[\"node1\", \"node2\"]].values.tolist()]\n",
    "    params = {\n",
    "    \"pairs\": pairs,\n",
    "    \"partitionProp\": partition_prop,\n",
    "    \"louvainProp\": louvain_prop\n",
    "    }\n",
    "    features = graph.run(query, params).to_data_frame()\n",
    "    features = features.drop_duplicates()\n",
    "    return pd.merge(data, features, on = [\"node1\", \"node2\"])\n",
    "\n",
    "# MSCALPELLO compare surname ids and return a 1 if the same\n",
    "# MSCALPELLO Modified to use connectionid and an appropriate index\n",
    "# Using NEO4J IDs does not work consistently\n",
    "def samename_features(data, samename_prop):\n",
    "    query = \"\"\"\n",
    "    UNWIND $pairs AS pair\n",
    "    MATCH (p1:Person) \n",
    "    USING INDEX p1:Person(connectionid) \n",
    "    WHERE p1.connectionid = pair.node1\n",
    "    MATCH (p2:Person) \n",
    "    USING INDEX p2:Person(connectionid) \n",
    "    WHERE p2.connectionid = pair.node2\n",
    "    RETURN pair.node1 AS node1,\n",
    "    pair.node2 AS node2,\n",
    "    algo.linkprediction.sameCommunity(p1, p2, $samenameProp) AS sn\n",
    "\"\"\"\n",
    "    pairs = [{\"node1\": node1, \"node2\": node2}  for node1,node2 in data[[\"node1\", \"node2\"]].values.tolist()]\n",
    "    params = {\n",
    "    \"pairs\": pairs,\n",
    "    \"samenameProp\": samename_prop\n",
    "    }\n",
    "    features = graph.run(query, params).to_data_frame()\n",
    "    features = features.drop_duplicates()\n",
    "    return pd.merge(data, features, on = [\"node1\", \"node2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSCALPELLO Use ExtraTrees classifier\n",
    "# Create the classifier\n",
    "        \n",
    "rfclassifier = ExtraTreesClassifier(n_estimators=1000, max_depth=50, \n",
    "                                  random_state=0,class_weight=\"balanced\")\n",
    "\n",
    "# Feature column names for a full link prediction features\n",
    "columns = [\"cn\",\"aa\",\"pa\", \"tn\",\"ra\", # link prediction features\n",
    "    \"lp\",\"lv\",                        # Community detection features\n",
    "    \"minT\", \"maxT\", \"minC\", \"maxC\",   # triangle features\n",
    "    \"sn\"                              # same name feature \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MSCALPELLO\n",
    "# TRAINING\n",
    "# Find positive links -Training data\n",
    "train_positive_links = graph.run(\"\"\"\n",
    "MATCH (p1:Person)-[:SUB_GRAPH_TRAINING]->(p2:Person)\n",
    "RETURN p1.connectionid AS node1, p2.connectionid AS node2, 1 AS label\n",
    "\"\"\").to_data_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MSCALPELLO\n",
    "# TEST\n",
    "# Find positive links - Test data\n",
    "test_positive_links = graph.run(\"\"\"\n",
    "MATCH (p1:Person)-[:SUB_GRAPH_TEST]->(p2:Person)\n",
    "RETURN p1.connectionid AS node1, p2.connectionid AS node2, 1 AS label\n",
    "\"\"\").to_data_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MSCALPELLO\n",
    "# TRAINING\n",
    "# Find negative links - Training data\n",
    "train_negative_links = graph.run(\"\"\"\n",
    "MATCH (p1:Person)\n",
    "WHERE (p1)-[:SUB_GRAPH_TRAINING]-()\n",
    "MATCH (p1)-[:SUB_GRAPH_TRAINING*2..3]-(p2)\n",
    "WHERE not((p1)-[:SUB_GRAPH_TRAINING]-(p2))\n",
    "RETURN p1.connectionid AS node1, p2.connectionid AS node2, 0 AS label\n",
    "\"\"\").to_data_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MSCALPELLO\n",
    "# TEST\n",
    "# Find negative examples - test set\n",
    "test_negative_links = graph.run(\"\"\"\n",
    "MATCH (p1:Person)\n",
    "WHERE (p1)-[:SUB_GRAPH_TEST]-()\n",
    "MATCH (p1)-[:SUB_GRAPH_TEST*2..3]-(p2)\n",
    "WHERE not((p1)-[:SUB_GRAPH_TEST]-(p2))\n",
    "RETURN p1.connectionid AS node1, p2.connectionid AS node2, 0 AS label\n",
    "\"\"\").to_data_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MSCALPELLO\n",
    "# TRAINING\n",
    "# Remove duplicates - training set\n",
    "train_negative_links = train_negative_links.drop_duplicates()\n",
    "\n",
    "#Down sample Negative links\n",
    "training_df = train_negative_links.append(train_positive_links, ignore_index=True)\n",
    "training_df['label'] = training_df['label'].astype('category')\n",
    "training_df = down_sample(training_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST\n",
    "# Remove duplicates - test set\n",
    "test_negative_links = test_negative_links.drop_duplicates()\n",
    "\n",
    "# Create DataFrame from positive and negative examples\n",
    "test_df = test_negative_links.append(test_positive_links, ignore_index=True)\n",
    "test_df['label'] = test_df['label'].astype('category')\n",
    "# Down sample negative examples\n",
    "test_df = down_sample(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TRAINING\n",
    "# Apply the link prediction features to the data set\n",
    "training_df = create_LP_features(training_df, \"SUB_GRAPH_TRAINING\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST\n",
    "# Apply the link prediction features to the data set\n",
    "test_df = create_LP_features(test_df, \"SUB_GRAPH_TEST\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#MSCALPELLO - Added iterations on LP to ensure it will converge\n",
    "# TRAINING\n",
    "# Community detection - label propogation\n",
    "graph.run(\"\"\"\n",
    "CALL algo.labelPropagation(\"Person\", \"SUB_GRAPH_TRAINING\", \"BOTH\",\n",
    "{iterations:20, partitionProperty: \"partitionTrain\"});\n",
    "\"\"\").to_data_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MSCALPELLO - Added iterations on LP to ensure it will converge\n",
    "# TEST\n",
    "# Community detection - label propogation\n",
    "graph.run(\"\"\"\n",
    "CALL algo.labelPropagation(\"Person\", \"SUB_GRAPH_TEST\", \"BOTH\",\n",
    "{iterations:20, partitionProperty: \"partitionTest\"});\n",
    "\"\"\").to_data_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#MSCALPELLO\n",
    "# TRAINING\n",
    "# Community detection - Louvain\n",
    "graph.run(\"\"\"\n",
    "CALL algo.louvain.stream(\"Person\", \"SUB_GRAPH_TRAINING\", {includeIntermediateCommunities:true})\n",
    "YIELD nodeId, community, communities\n",
    "WITH algo.getNodeById(nodeId) AS node, community AS smallestCommunity\n",
    "SET node.louvainTrain = smallestCommunity;\n",
    "\"\"\").stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MSCALPELLO\n",
    "# TEST\n",
    "# Community detection - Louvain\n",
    "graph.run(\"\"\"\n",
    "CALL algo.louvain.stream(\"Person\", \"SUB_GRAPH_TEST\", {includeIntermediateCommunities:true})\n",
    "YIELD nodeId, community, communities\n",
    "WITH algo.getNodeById(nodeId) AS node, community AS smallestCommunity\n",
    "SET node.louvainTest = smallestCommunity;\n",
    "\"\"\").stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the community detection algorithm to find the label prop and louvain features seeded into the graph\n",
    "# TRAINING\n",
    "training_df = communitydetection_features(training_df, \"partitionTrain\", \"louvainTrain\")\n",
    "#TEST\n",
    "test_df = communitydetection_features(test_df, \"partitionTest\", \"louvainTest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.run(\"\"\"\n",
    "CALL algo.triangleCount('Person', 'SUB_GRAPH_TRAINING', { write:true,\n",
    "writeProperty:'trianglesTrain', clusteringCoefficientProperty:'coefficientTrain'});\n",
    "\"\"\").to_data_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.run(\"\"\"\n",
    "CALL algo.triangleCount('Person', 'SUB_GRAPH_TEST', { write:true,\n",
    "writeProperty:'trianglesTest', clusteringCoefficientProperty:'coefficientTest'});\n",
    "\"\"\").to_data_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df = create_community_features(training_df, \"trianglesTrain\", \"coefficientTrain\")\n",
    "test_df = create_community_features(test_df, \"trianglesTest\", \"coefficientTest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MSCALPELLO See if node pair has the same surname\n",
    "#TRAINING\n",
    "training_df = samename_features(training_df, \"surnameID\")\n",
    "#TEST\n",
    "test_df = samename_features(test_df, \"surnameID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the classifier against the training data\n",
    "X = training_df[columns]\n",
    "y = training_df[\"label\"]\n",
    "rfclassifier.fit(X, y)\n",
    "\n",
    "# Apply the test data and evaluate performance\n",
    "predictions = rfclassifier.predict(test_df[columns])\n",
    "y_test = test_df[\"label\"]\n",
    "\n",
    "display(evaluate_model(predictions, y_test))\n",
    "feature_importance(columns, rfclassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MSCALPELLO\n",
    "#File path for model\n",
    "filename = 'File path eher V5.sav'\n",
    "pickle.dump(rfclassifier, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
